{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07597e48",
   "metadata": {},
   "source": [
    "### Setup and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcd32656",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/05 07:35:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark gestart via Polaris\n",
      "üî¢ Spark versie: 3.5.1\n",
      "üì¶ Default catalog: polaris\n",
      "üß≠ Polaris URI: http://polaris:8181\n",
      "üè≠ Polaris warehouse: polaris\n",
      "üéØ Polaris scope: PRINCIPAL_ROLE:ALL\n",
      "üåê S3A endpoint: http://minio:9000\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 00_setup_and_test.ipynb\n",
    "# Spark + Iceberg + MinIO via Polaris REST catalog\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Polaris credentials (are generated into .env by the bootstrap container)\n",
    "POLARIS_CLIENT_ID = os.getenv(\"POLARIS_CLIENT_ID\")\n",
    "POLARIS_CLIENT_SECRET = os.getenv(\"POLARIS_CLIENT_SECRET\")\n",
    "\n",
    "if not POLARIS_CLIENT_ID or not POLARIS_CLIENT_SECRET:\n",
    "    raise RuntimeError(\n",
    "        \"‚ùå Polaris credentials ontbreken. Zet POLARIS_CLIENT_ID en POLARIS_CLIENT_SECRET in de omgeving.\"\n",
    "    )\n",
    "\n",
    "# Polaris endpoints + scope\n",
    "POLARIS_URI = os.getenv(\"POLARIS_URI\", \"http://polaris:8181/api/catalog\")\n",
    "POLARIS_OAUTH2 = os.getenv(\n",
    "    \"POLARIS_OAUTH2_TOKEN_URL\", \"http://polaris:8181/api/catalog/v1/oauth/tokens\"\n",
    ")\n",
    "POLARIS_SCOPE = os.getenv(\"POLARIS_SCOPE\", \"PRINCIPAL_ROLE:ALL\")\n",
    "POLARIS_WAREHOUSE = os.getenv(\"POLARIS_CATALOG_NAME\", \"polaris\")\n",
    "\n",
    "# === 0Ô∏è‚É£ Oude Spark-sessie stoppen (veilig) ===\n",
    "if \"spark\" in locals():\n",
    "    try:\n",
    "        spark.stop()\n",
    "        print(\"üßπ Oude Spark-sessie gestopt.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Kon Spark niet netjes stoppen: {e}\")\n",
    "\n",
    "# === 1Ô∏è‚É£ SparkSession met Iceberg Polaris REST Catalog ===\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"Lakehouse-Unplugged\")\n",
    "\n",
    "        # Iceberg\n",
    "        .config(\n",
    "            \"spark.sql.extensions\",\n",
    "            \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\"\n",
    "        )\n",
    "\n",
    "        # Polaris REST catalog\n",
    "        .config(\"spark.sql.defaultCatalog\", \"polaris\")\n",
    "        .config(\"spark.sql.catalog.polaris\", \"org.apache.polaris.spark.SparkCatalog\")\n",
    "        .config(\"spark.sql.catalog.polaris.uri\", POLARIS_URI)\n",
    "        .config(\"spark.sql.catalog.polaris.warehouse\", POLARIS_WAREHOUSE)\n",
    "        .config(\n",
    "            \"spark.sql.catalog.polaris.credential\",\n",
    "            f\"{POLARIS_CLIENT_ID}:{POLARIS_CLIENT_SECRET}\"\n",
    "        )\n",
    "        .config(\"spark.sql.catalog.polaris.oauth2-server-uri\", POLARIS_OAUTH2)\n",
    "        .config(\"spark.sql.catalog.polaris.scope\", POLARIS_SCOPE)\n",
    "        .config(\n",
    "            \"spark.sql.catalog.polaris.header.X-Iceberg-Access-Delegation\",\n",
    "            \"vended-credentials\",\n",
    "        )\n",
    "        .config(\"spark.sql.catalog.polaris.token-refresh-enabled\", \"true\")\n",
    "\n",
    "        # Performance (licht, PoC)\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "\n",
    "        # S3A / MinIO voor de data-files\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "# === 2Ô∏è‚É£ Validatie ===\n",
    "hconf = spark._jsc.hadoopConfiguration()\n",
    "\n",
    "print(\"‚úÖ Spark gestart via Polaris\")\n",
    "print(f\"üî¢ Spark versie: {spark.version}\")\n",
    "print(f\"üì¶ Default catalog: {spark.conf.get('spark.sql.defaultCatalog')}\")\n",
    "print(f\"üß≠ Polaris URI: {spark.conf.get('spark.sql.catalog.polaris.uri')}\")\n",
    "print(f\"üè≠ Polaris warehouse: {spark.conf.get('spark.sql.catalog.polaris.warehouse')}\")\n",
    "print(f\"üéØ Polaris scope: {spark.conf.get('spark.sql.catalog.polaris.scope')}\")\n",
    "print(f\"üåê S3A endpoint: {hconf.get('fs.s3a.endpoint')}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Controleer de setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c228e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Controle: Polaris REST + MinIO via S3A...\n",
      "üß≠ Polaris URI: http://polaris:8181\n",
      "üè≠ Polaris warehouse: polaris\n",
      "üéØ Polaris scope: PRINCIPAL_ROLE:ALL\n",
      "üåê S3A endpoint: http://minio:9000\n",
      "‚öôÔ∏è  Implementation: org.apache.hadoop.fs.s3a.S3AFileSystem\n",
      "üîë Access key: minioadmin\n",
      "üì° Probe: probeer S3A pad te controleren...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/05 07:36:28 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Verbinding OK ‚Äî 'warehouse' bucket is bereikbaar via S3A.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "print(\"üîç Controle: Polaris REST + MinIO via S3A...\")\n",
    "\n",
    "try:\n",
    "    # Polaris-config uit Spark\n",
    "    catalog_uri = spark.conf.get(\"spark.sql.catalog.polaris.uri\")\n",
    "    warehouse = spark.conf.get(\"spark.sql.catalog.polaris.warehouse\")\n",
    "    scope = spark.conf.get(\"spark.sql.catalog.polaris.scope\")\n",
    "\n",
    "    print(f\"üß≠ Polaris URI: {catalog_uri}\")\n",
    "    print(f\"üè≠ Polaris warehouse: {warehouse}\")\n",
    "    print(f\"üéØ Polaris scope: {scope}\")\n",
    "\n",
    "    # Hadoop-config uit Spark voor MinIO I/O\n",
    "    conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "    endpoint = conf.get(\"fs.s3a.endpoint\")\n",
    "    impl = conf.get(\"fs.s3a.impl\")\n",
    "    access_key = conf.get(\"fs.s3a.access.key\")\n",
    "\n",
    "    print(f\"üåê S3A endpoint: {endpoint}\")\n",
    "    print(f\"‚öôÔ∏è  Implementation: {impl}\")\n",
    "    print(f\"üîë Access key: {access_key}\")\n",
    "\n",
    "    print(\"üì° Probe: probeer S3A pad te controleren...\")\n",
    "\n",
    "    # Test pad: je daadwerkelijke warehouse in MinIO\n",
    "    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(\n",
    "        spark._jvm.java.net.URI(\"s3a://warehouse/\"), conf\n",
    "    )\n",
    "    path = spark._jvm.org.apache.hadoop.fs.Path(\"s3a://warehouse/\")\n",
    "\n",
    "    if fs.exists(path):\n",
    "        print(\"‚úÖ Verbinding OK ‚Äî 'warehouse' bucket is bereikbaar via S3A.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Verbinding OK, maar 'warehouse' bucket bestaat nog niet.\")\n",
    "        print(\"   ‚ûú Spark maakt deze automatisch aan bij de eerste write.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Fout bij verbinding met Polaris/MinIO:\")\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parkeer bestande in de landingzone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64d4a7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Lokaal bestand: /workspace/data/gekentekendevoertuigen_sample.json\n",
      "‚¨ÜÔ∏è Upload naar:  s3a://warehouse/landing/gekentekendevoertuigen_sample.json\n",
      "‚úÖ Upload gelukt.\n",
      "üì¶ Objecten in MinIO:\n",
      " - landing/gekentekendevoertuigen_sample.json\n",
      "üì• Inlezen via Spark: s3a://warehouse/landing/gekentekendevoertuigen_sample.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o69.count.\n: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'polaris': org.apache.polaris.spark.SparkCatalog.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1925)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:53)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:53)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:122)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:93)\n\tat org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)\n\tat org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:295)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:295)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:275)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)\n\tat scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)\n\tat org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:3613)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: org.apache.polaris.spark.SparkCatalog\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)\n\t... 58 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 63\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müì• Inlezen via Spark: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms3_uri\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     61\u001b[39m df = spark.read.option(\u001b[33m\"\u001b[39m\u001b[33mmultiline\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m).json(s3_uri)\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìä Aantal records: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     64\u001b[39m df.printSchema()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/sql/dataframe.py:1240\u001b[39m, in \u001b[36mDataFrame.count\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1217\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m   1218\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[32m   1219\u001b[39m \n\u001b[32m   1220\u001b[39m \u001b[33;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1238\u001b[39m \u001b[33;03m    3\u001b[39;00m\n\u001b[32m   1239\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o69.count.\n: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'polaris': org.apache.polaris.spark.SparkCatalog.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1925)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:53)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:53)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:122)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:93)\n\tat org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)\n\tat org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:295)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:295)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:275)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)\n\tat scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)\n\tat org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:3613)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: org.apache.polaris.spark.SparkCatalog\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)\n\t... 58 more\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from pathlib import Path\n",
    "\n",
    "# ======================================================================\n",
    "# 0Ô∏è‚É£ Helper: zoek automatisch lokaal data-bestand\n",
    "# ======================================================================\n",
    "def find_data_file(filename: str) -> Path:\n",
    "    p = Path.cwd()\n",
    "    for _ in range(4):\n",
    "        candidate = p / \"data\" / filename\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "        p = p.parent\n",
    "    raise FileNotFoundError(f\"‚ùå Kon '{filename}' niet vinden in een 'data' map.\")\n",
    "\n",
    "# ======================================================================\n",
    "# 1Ô∏è‚É£ Config\n",
    "# ======================================================================\n",
    "local_file = find_data_file(\"gekentekendevoertuigen_sample.json\")\n",
    "bucket = \"warehouse\"\n",
    "prefix = \"landing\"\n",
    "\n",
    "object_key = f\"{prefix}/{local_file.name}\"\n",
    "s3_uri = f\"s3a://{bucket}/{object_key}\"\n",
    "\n",
    "print(f\"üìÑ Lokaal bestand: {local_file}\")\n",
    "print(f\"‚¨ÜÔ∏è Upload naar:  {s3_uri}\")\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# 2Ô∏è‚É£ MinIO client via boto3\n",
    "# ======================================================================\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=\"http://minio:9000\",\n",
    "    aws_access_key_id=\"minioadmin\",\n",
    "    aws_secret_access_key=\"minioadmin\",\n",
    "    region_name=\"us-east-1\",\n",
    ")\n",
    "\n",
    "# Upload bestand\n",
    "s3.upload_file(str(local_file), bucket, object_key)\n",
    "\n",
    "print(\"‚úÖ Upload gelukt.\")\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# 3Ô∏è‚É£ Verify: lijst objecten in prefix\n",
    "# ======================================================================\n",
    "response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "print(\"üì¶ Objecten in MinIO:\")\n",
    "for item in response.get(\"Contents\", []):\n",
    "    print(\" -\", item[\"Key\"])\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# 4Ô∏è‚É£ Spark read via S3A (data-files), metadata via Polaris\n",
    "# ======================================================================\n",
    "print(f\"üì• Inlezen via Spark: {s3_uri}\")\n",
    "\n",
    "df = spark.read.option(\"multiline\", \"true\").json(s3_uri)\n",
    "\n",
    "print(f\"üìä Aantal records: {df.count():,}\")\n",
    "df.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ingest into bronze table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Lezen vanuit landingzone: s3a://warehouse/landing/gekentekendevoertuigen_sample.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Aantal records geladen: 10002\n",
      "üßä Schrijven naar Bronze tabel: lakehouse.bronze.gekentekendevoertuigen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/22 10:28:52 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/11/22 10:28:55 WARN HadoopTableOperations: Error reading version hint file s3a://warehouse/bronze/gekentekendevoertuigen/metadata/version-hint.text\n",
      "java.io.FileNotFoundException: No such file or directory: s3a://warehouse/bronze/gekentekendevoertuigen/metadata/version-hint.text\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3866)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.extractOrFetchSimpleFileStatus(S3AFileSystem.java:5401)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1465)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1441)\n",
      "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.findVersion(HadoopTableOperations.java:318)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.refresh(HadoopTableOperations.java:104)\n",
      "\tat org.apache.iceberg.BaseTransaction.lambda$commitReplaceTransaction$1(BaseTransaction.java:368)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitReplaceTransaction(BaseTransaction.java:365)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitTransaction(BaseTransaction.java:314)\n",
      "\tat org.apache.iceberg.CommitCallbackTransaction.commitTransaction(CommitCallbackTransaction.java:126)\n",
      "\tat org.apache.iceberg.spark.source.StagedSparkTable.commitStagedChanges(StagedSparkTable.java:34)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:580)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:573)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:567)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:183)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:216)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:196)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.internalReplace(DataFrameWriterV2.scala:208)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.createOrReplace(DataFrameWriterV2.scala:134)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/11/22 10:28:55 WARN HadoopTableOperations: Error reading version hint file s3a://warehouse/bronze/gekentekendevoertuigen/metadata/version-hint.text\n",
      "java.io.FileNotFoundException: No such file or directory: s3a://warehouse/bronze/gekentekendevoertuigen/metadata/version-hint.text\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3866)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.extractOrFetchSimpleFileStatus(S3AFileSystem.java:5401)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1465)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1441)\n",
      "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.findVersion(HadoopTableOperations.java:318)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.refresh(HadoopTableOperations.java:104)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.current(HadoopTableOperations.java:84)\n",
      "\tat org.apache.iceberg.BaseTransaction.lambda$commitReplaceTransaction$1(BaseTransaction.java:377)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitReplaceTransaction(BaseTransaction.java:365)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitTransaction(BaseTransaction.java:314)\n",
      "\tat org.apache.iceberg.CommitCallbackTransaction.commitTransaction(CommitCallbackTransaction.java:126)\n",
      "\tat org.apache.iceberg.spark.source.StagedSparkTable.commitStagedChanges(StagedSparkTable.java:34)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:580)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:573)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:567)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:183)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:216)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:196)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.internalReplace(DataFrameWriterV2.scala:208)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.createOrReplace(DataFrameWriterV2.scala:134)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bronze tabel bijgewerkt: lakehouse.bronze.gekentekendevoertuigen\n",
      "\n",
      "üìã Tabellen in lakehouse.bronze:\n",
      "+---------+----------------------+-----------+\n",
      "|namespace|tableName             |isTemporary|\n",
      "+---------+----------------------+-----------+\n",
      "|bronze   |gekentekendevoertuigen|false      |\n",
      "+---------+----------------------+-----------+\n",
      "\n",
      "\n",
      "üîÅ Records in Bronze: 10,002\n",
      "+---------------+----------------------------+----------------------------+----------------+-------------+-----------------------+--------------------+-------------+------------------+-----------------------------------------------+---------------------------------------------+---------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------------+-------------------------------------------------+-----------------------------------------------+-------+------------------------+------------------------+---------+--------------+--------------+-----------------------------------+----------------------------------------+-------------------------------------------+----------------------+-------------------------+--------------------+-----------------------+------------+---------------------------------------+--------------------------+-------------------------------------+----------------+---------------+---------------+-----------------------+-----------------------+--------------------+------------------------------------+--------+------------+------+-----------------------+-----------------------+----------------------------+----------------------------+--------------------+--------------+----------------------------+--------------------------------------------------+---------------------------+--------------------------------+--------------------------------+------------------------------+-------------------------------+----------------------------+---------+------------------------------------+---------------+-----------------------------------+-----------------------------------------------------+--------------------------------------------------------+----------------------+--------------+-----------------------------+------------------+----------------------+---------------------------------+------------------+-----+-------------------+----------------------+--------------------+-------+----------------------+-------------------------------------------------+---------------+-----------------------+----------------------+-------------------------+-------------+---------------------------------------+------------------------------+-------------+---------+--------------------------+--------------------------+------------------------+\n",
      "|_corrupt_record|aanhangwagen_autonoom_geremd|aanhangwagen_middenas_geremd|aantal_cilinders|aantal_deuren|aantal_rolstoelplaatsen|aantal_staanplaatsen|aantal_wielen|aantal_zitplaatsen|afstand_hart_koppeling_tot_achterzijde_voertuig|afstand_voorzijde_voertuig_tot_hart_koppeling|afwijkende_maximum_snelheid|api_gekentekende_voertuigen_assen              |api_gekentekende_voertuigen_brandstof          |api_gekentekende_voertuigen_carrosserie        |api_gekentekende_voertuigen_carrosserie_specifiek|api_gekentekende_voertuigen_voertuigklasse     |breedte|breedte_voertuig_maximum|breedte_voertuig_minimum|bruto_bpm|catalogusprijs|cilinderinhoud|code_toelichting_tellerstandoordeel|datum_eerste_tenaamstelling_in_nederland|datum_eerste_tenaamstelling_in_nederland_dt|datum_eerste_toelating|datum_eerste_toelating_dt|datum_tenaamstelling|datum_tenaamstelling_dt|eerste_kleur|europese_uitvoeringcategorie_toevoeging|europese_voertuigcategorie|europese_voertuigcategorie_toevoeging|export_indicator|handelsbenaming|hoogte_voertuig|hoogte_voertuig_maximum|hoogte_voertuig_minimum|inrichting          |jaar_laatste_registratie_tellerstand|kenteken|laadvermogen|lengte|lengte_voertuig_maximum|lengte_voertuig_minimum|massa_bedrijfsklaar_maximaal|massa_bedrijfsklaar_minimaal|massa_ledig_voertuig|massa_rijklaar|maximale_constructiesnelheid|maximum_last_onder_de_vooras_sen_tezamen_koppeling|maximum_massa_samenstelling|maximum_massa_technisch_maximaal|maximum_massa_technisch_minimaal|maximum_massa_trekken_ongeremd|maximum_ondersteunende_snelheid|maximum_trekken_massa_geremd|merk     |openstaande_terugroepactie_indicator|oplegger_geremd|plaats_chassisnummer               |registratie_datum_goedkeuring_afschrijvingsmoment_bpm|registratie_datum_goedkeuring_afschrijvingsmoment_bpm_dt|subcategorie_nederland|taxi_indicator|technische_max_massa_voertuig|tellerstandoordeel|tenaamstellen_mogelijk|toegestane_maximum_massa_voertuig|tweede_kleur      |type |type_gasinstallatie|typegoedkeuringsnummer|uitvoering          |variant|vermogen_massarijklaar|verticale_belasting_koppelpunt_getrokken_voertuig|vervaldatum_apk|vervaldatum_apk_dt     |vervaldatum_tachograaf|vervaldatum_tachograaf_dt|voertuigsoort|volgnummer_wijziging_eu_typegoedkeuring|wacht_op_keuren               |wam_verzekerd|wielbasis|wielbasis_voertuig_maximum|wielbasis_voertuig_minimum|zuinigheidsclassificatie|\n",
      "+---------------+----------------------------+----------------------------+----------------+-------------+-----------------------+--------------------+-------------+------------------+-----------------------------------------------+---------------------------------------------+---------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------------+-------------------------------------------------+-----------------------------------------------+-------+------------------------+------------------------+---------+--------------+--------------+-----------------------------------+----------------------------------------+-------------------------------------------+----------------------+-------------------------+--------------------+-----------------------+------------+---------------------------------------+--------------------------+-------------------------------------+----------------+---------------+---------------+-----------------------+-----------------------+--------------------+------------------------------------+--------+------------+------+-----------------------+-----------------------+----------------------------+----------------------------+--------------------+--------------+----------------------------+--------------------------------------------------+---------------------------+--------------------------------+--------------------------------+------------------------------+-------------------------------+----------------------------+---------+------------------------------------+---------------+-----------------------------------+-----------------------------------------------------+--------------------------------------------------------+----------------------+--------------+-----------------------------+------------------+----------------------+---------------------------------+------------------+-----+-------------------+----------------------+--------------------+-------+----------------------+-------------------------------------------------+---------------+-----------------------+----------------------+-------------------------+-------------+---------------------------------------+------------------------------+-------------+---------+--------------------------+--------------------------+------------------------+\n",
      "|[              |NULL                        |NULL                        |NULL            |NULL         |NULL                   |NULL                |NULL         |NULL              |NULL                                           |NULL                                         |NULL                       |NULL                                           |NULL                                           |NULL                                           |NULL                                             |NULL                                           |NULL   |NULL                    |NULL                    |NULL     |NULL          |NULL          |NULL                               |NULL                                    |NULL                                       |NULL                  |NULL                     |NULL                |NULL                   |NULL        |NULL                                   |NULL                      |NULL                                 |NULL            |NULL           |NULL           |NULL                   |NULL                   |NULL                |NULL                                |NULL    |NULL        |NULL  |NULL                   |NULL                   |NULL                        |NULL                        |NULL                |NULL          |NULL                        |NULL                                              |NULL                       |NULL                            |NULL                            |NULL                          |NULL                           |NULL                        |NULL     |NULL                                |NULL           |NULL                               |NULL                                                 |NULL                                                    |NULL                  |NULL          |NULL                         |NULL              |NULL                  |NULL                             |NULL              |NULL |NULL               |NULL                  |NULL                |NULL   |NULL                  |NULL                                             |NULL           |NULL                   |NULL                  |NULL                     |NULL         |NULL                                   |NULL                          |NULL         |NULL     |NULL                      |NULL                      |NULL                    |\n",
      "|NULL           |NULL                        |NULL                        |NULL            |NULL         |NULL                   |NULL                |NULL         |NULL              |419                                            |NULL                                         |NULL                       |https://opendata.rdw.nl/resource/3huj-srit.json|https://opendata.rdw.nl/resource/8ys7-d773.json|https://opendata.rdw.nl/resource/vezc-m2t6.json|https://opendata.rdw.nl/resource/jhie-znh9.json  |https://opendata.rdw.nl/resource/kmfi-hrps.json|174    |NULL                    |NULL                    |NULL     |NULL          |NULL          |NG                                 |19780914                                |1978-09-14T00:00:00.000                    |19780914              |1978-09-14T00:00:00.000  |20221201            |2022-12-01T00:00:00.000|N.v.t.      |NULL                                   |O2                        |NULL                                 |Nee             |P.T.A.-1500    |NULL           |NULL                   |NULL                   |open wagen          |NULL                                |0039WG  |1090        |NULL  |NULL                   |NULL                   |NULL                        |NULL                        |410                 |NULL          |100                         |0                                                 |NULL                       |NULL                            |NULL                            |NULL                          |NULL                           |NULL                        |PIJNAPPEL|Nee                                 |NULL           |tegen r. balk 130 cm v. hart asstel|NULL                                                 |NULL                                                    |NULL                  |Nee           |1500                         |Niet geregistreerd|Ja                    |1500                             |N.v.t.            |NULL |NULL               |NULL                  |NULL                |NULL   |NULL                  |NULL                                             |NULL           |NULL                   |NULL                  |NULL                     |Aanhangwagen |NULL                                   |Geen verstrekking in Open Data|N.v.t.       |NULL     |NULL                      |NULL                      |NULL                    |\n",
      "|NULL           |28928                       |14369                       |6               |NULL         |NULL                   |NULL                |8            |2                 |NULL                                           |908                                          |NULL                       |https://opendata.rdw.nl/resource/3huj-srit.json|https://opendata.rdw.nl/resource/8ys7-d773.json|https://opendata.rdw.nl/resource/vezc-m2t6.json|https://opendata.rdw.nl/resource/jhie-znh9.json  |https://opendata.rdw.nl/resource/kmfi-hrps.json|255    |NULL                    |NULL                    |NULL     |NULL          |10837         |NG                                 |20160325                                |2016-03-25T00:00:00.000                    |20160325              |2016-03-25T00:00:00.000  |20221201            |2022-12-01T00:00:00.000|N.v.t.      |NULL                                   |N3                        |NULL                                 |Nee             |XF 440 FAN     |NULL           |NULL                   |NULL                   |afneembare bovenbouw|NULL                                |00BHB9  |17155       |1003  |NULL                   |NULL                   |NULL                        |NULL                        |9845                |9945          |NULL                        |0                                                 |50000                      |NULL                            |NULL                            |750                           |NULL                           |NULL                        |DAF      |Nee                                 |NULL           |r. vooras                          |NULL                                                 |NULL                                                    |NULL                  |Nee           |27000                        |Niet geregistreerd|Ja                    |27000                            |N.v.t.            |H4SN3|NULL               |NULL                  |NULL                |NULL   |0.03                  |NULL                                             |20231211       |2023-12-11T00:00:00.000|20241201              |2024-12-01T00:00:00.000  |Bedrijfsauto |NULL                                   |Geen verstrekking in Open Data|Ja           |670      |NULL                      |NULL                      |NULL                    |\n",
      "|NULL           |NULL                        |NULL                        |6               |2            |0                      |NULL                |6            |2                 |0                                              |459                                          |NULL                       |https://opendata.rdw.nl/resource/3huj-srit.json|https://opendata.rdw.nl/resource/8ys7-d773.json|https://opendata.rdw.nl/resource/vezc-m2t6.json|https://opendata.rdw.nl/resource/jhie-znh9.json  |https://opendata.rdw.nl/resource/kmfi-hrps.json|255    |NULL                    |NULL                    |NULL     |NULL          |12777         |NG                                 |20221125                                |2022-11-25T00:00:00.000                    |20220927              |2022-09-27T00:00:00.000  |20221201            |2022-12-01T00:00:00.000|N.v.t.      |NULL                                   |N3                        |NULL                                 |Nee             |FH             |NULL           |NULL                   |NULL                   |opleggertrekker     |NULL                                |00BTR3  |NULL        |621   |NULL                   |NULL                   |NULL                        |NULL                        |8484                |8584          |90                          |NULL                                              |44000                      |NULL                            |NULL                            |NULL                          |0.00                           |NULL                        |VOLVO    |Nee                                 |35416          |r. balk by vooras                  |20221115                                             |2022-11-15T00:00:00.000                                 |NULL                  |Nee           |20500                        |Niet geregistreerd|Ja                    |19000                            |N.v.t.            |VTA3T|NULL               |e5*2007/46*1013*12    |N5RR3S67513XX3M5TUUC|C3FHA1 |0.04                  |NULL                                             |20230927       |2023-09-27T00:00:00.000|20241125              |2024-11-25T00:00:00.000  |Bedrijfsauto |0                                      |Geen verstrekking in Open Data|Ja           |380      |NULL                      |NULL                      |NULL                    |\n",
      "|NULL           |NULL                        |NULL                        |4               |2            |0                      |NULL                |4            |5                 |0                                              |0                                            |NULL                       |https://opendata.rdw.nl/resource/3huj-srit.json|https://opendata.rdw.nl/resource/8ys7-d773.json|https://opendata.rdw.nl/resource/vezc-m2t6.json|https://opendata.rdw.nl/resource/jhie-znh9.json  |https://opendata.rdw.nl/resource/kmfi-hrps.json|0      |NULL                    |NULL                    |2152     |14469         |1396          |00                                 |20090617                                |2009-06-17T00:00:00.000                    |20090617              |2009-06-17T00:00:00.000  |20221201            |2022-12-01T00:00:00.000|GRIJS       |NULL                                   |M1                        |NULL                                 |Nee             |I20            |NULL           |NULL                   |NULL                   |MPV                 |2022                                |00JJT8  |NULL        |394   |NULL                   |NULL                   |NULL                        |NULL                        |1000                |1100          |NULL                        |NULL                                              |2565                       |NULL                            |NULL                            |450                           |NULL                           |1000                        |HYUNDAI  |Nee                                 |NULL           |r. dwarsbalk by voorzitting        |NULL                                                 |NULL                                                    |NULL                  |Nee           |1565                         |Logisch           |Ja                    |1565                             |Niet geregistreerd|PB   |NULL               |e11*2001/116*0333*01  |M52AY1              |F5P21  |0.07                  |NULL                                             |20230317       |2023-03-17T00:00:00.000|NULL                  |NULL                     |Personenauto |0                                      |Geen verstrekking in Open Data|Ja           |253      |NULL                      |NULL                      |B                       |\n",
      "+---------------+----------------------------+----------------------------+----------------+-------------+-----------------------+--------------------+-------------+------------------+-----------------------------------------------+---------------------------------------------+---------------------------+-----------------------------------------------+-----------------------------------------------+-----------------------------------------------+-------------------------------------------------+-----------------------------------------------+-------+------------------------+------------------------+---------+--------------+--------------+-----------------------------------+----------------------------------------+-------------------------------------------+----------------------+-------------------------+--------------------+-----------------------+------------+---------------------------------------+--------------------------+-------------------------------------+----------------+---------------+---------------+-----------------------+-----------------------+--------------------+------------------------------------+--------+------------+------+-----------------------+-----------------------+----------------------------+----------------------------+--------------------+--------------+----------------------------+--------------------------------------------------+---------------------------+--------------------------------+--------------------------------+------------------------------+-------------------------------+----------------------------+---------+------------------------------------+---------------+-----------------------------------+-----------------------------------------------------+--------------------------------------------------------+----------------------+--------------+-----------------------------+------------------+----------------------+---------------------------------+------------------+-----+-------------------+----------------------+--------------------+-------+----------------------+-------------------------------------------------+---------------+-----------------------+----------------------+-------------------------+-------------+---------------------------------------+------------------------------+-------------+---------+--------------------------+--------------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# üîÑ Ingest van Landingzone ‚Üí Bronze (Iceberg via Polaris)\n",
    "# ======================================================================\n",
    "\n",
    "# Input configuratie (aangeleverd vanuit eerdere cel)\n",
    "bucket = \"warehouse\"\n",
    "prefix = \"landing\"\n",
    "local_file = find_data_file(\"gekentekendevoertuigen_sample.json\")   # naam komt uit je workflow\n",
    "object_key = f\"{prefix}/{local_file.name}\"\n",
    "s3_uri = f\"s3a://{bucket}/{object_key}\"\n",
    "\n",
    "bronze_table = \"polaris.bronze.gekentekendevoertuigen\"\n",
    "\n",
    "print(f\"üì• Lezen vanuit landingzone: {s3_uri}\")\n",
    "\n",
    "# 1Ô∏è‚É£ Data inlezen uit landingzone\n",
    "df = spark.read.json(s3_uri)\n",
    "\n",
    "print(f\"üì¶ Aantal records geladen: {df.count()}\")\n",
    "\n",
    "# 2Ô∏è‚É£ Namespace garanderen\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS polaris.bronze\")\n",
    "\n",
    "# 3Ô∏è‚É£ Wegschrijven naar Iceberg Bronze\n",
    "print(f\"üßä Schrijven naar Bronze tabel: {bronze_table}\")\n",
    "\n",
    "(\n",
    "    df.writeTo(bronze_table)\n",
    "      .using(\"iceberg\")\n",
    "      .option(\"format-version\", \"2\")\n",
    "      .option(\"overwrite-mode\", \"dynamic\")\n",
    "      .createOrReplace()\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Bronze tabel bijgewerkt: {bronze_table}\")\n",
    "\n",
    "# 4Ô∏è‚É£ Tabellen tonen\n",
    "print(\"\n",
    "üìã Tabellen in polaris.bronze:\")\n",
    "spark.sql(\"SHOW TABLES IN polaris.bronze\").show(truncate=False)\n",
    "\n",
    "# 5Ô∏è‚É£ Bronze teruglezen ter controle\n",
    "bronze_df = spark.read.table(bronze_table)\n",
    "\n",
    "print(f\"\n",
    "üîÅ Records in Bronze: {bronze_df.count():,}\")\n",
    "bronze_df.show(5, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query de bronze table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöó Top 5 voertuigsoorten:\n",
      "+--------------------+-----+\n",
      "|voertuigsoort       |count|\n",
      "+--------------------+-----+\n",
      "|Personenauto        |7078 |\n",
      "|Bedrijfsauto        |1237 |\n",
      "|Bromfiets           |782  |\n",
      "|Motorfiets          |258  |\n",
      "|Middenasaanhangwagen|136  |\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "üè∑Ô∏è Top 5 merken:\n",
      "+-------------+-----+\n",
      "|merk         |count|\n",
      "+-------------+-----+\n",
      "|VOLKSWAGEN   |1076 |\n",
      "|PEUGEOT      |615  |\n",
      "|RENAULT      |606  |\n",
      "|MERCEDES-BENZ|565  |\n",
      "|FORD         |553  |\n",
      "+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "üî§ Top 5 handelsbenamingen:\n",
      "+---------------+-----+\n",
      "|handelsbenaming|count|\n",
      "+---------------+-----+\n",
      "|POLO           |219  |\n",
      "|GOLF           |202  |\n",
      "|FOCUS          |138  |\n",
      "|N/A            |135  |\n",
      "|CLIO           |125  |\n",
      "+---------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "‚ö° Top 5 voertuigen op vermogen (massarijklaar):\n",
      "+-------+--------------------+----------------------+\n",
      "|merk   |handelsbenaming     |vermogen_massarijklaar|\n",
      "+-------+--------------------+----------------------+\n",
      "|DUCATI |PANIGALE V4         |0.79                  |\n",
      "|YAMAHA |YZF-R1              |0.74                  |\n",
      "|YAMAHA |YZF1000             |0.73                  |\n",
      "|TRIUMPH|SPEED TRIPLE 1200 RR|0.67                  |\n",
      "|SUZUKI |GSX-R1000           |0.66                  |\n",
      "+-------+--------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = spark.read.table(\"polaris.bronze.gekentekendevoertuigen\")\n",
    "\n",
    "print(\"üöó Top 5 voertuigsoorten:\")\n",
    "(\n",
    "    df.groupBy(\"voertuigsoort\")\n",
    "      .count()\n",
    "      .orderBy(col(\"count\").desc())\n",
    "      .show(5, truncate=False)\n",
    ")\n",
    "\n",
    "print(\"\n",
    "üè∑Ô∏è Top 5 merken:\")\n",
    "(\n",
    "    df.groupBy(\"merk\")\n",
    "      .count()\n",
    "      .orderBy(col(\"count\").desc())\n",
    "      .show(5, truncate=False)\n",
    ")\n",
    "\n",
    "print(\"\n",
    "üî§ Top 5 handelsbenamingen:\")\n",
    "(\n",
    "    df.groupBy(\"handelsbenaming\")\n",
    "      .count()\n",
    "      .orderBy(col(\"count\").desc())\n",
    "      .show(5, truncate=False)\n",
    ")\n",
    "\n",
    "print(\"\n",
    "‚ö° Top 5 voertuigen op vermogen (massarijklaar):\")\n",
    "(\n",
    "    df.select(\"merk\", \"handelsbenaming\", \"vermogen_massarijklaar\")\n",
    "      .orderBy(col(\"vermogen_massarijklaar\").desc_nulls_last())\n",
    "      .show(5, truncate=False)\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Lakehouse-Unplugged)",
   "language": "python",
   "name": "lakehouse-unplugged"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
